# Basic site crawler

This is a basic site crawler that visits websites either from a provided text file (urls.txt), an XML sitemap, or both. It checks if the DOM content is loaded and logs any issues encountered. Additionally, the crawler detects 404 Not Found errors, 500 Internal Server Errors, and analyzes page load performance metrics, saving the results in separate files.

## Installation

Install the necessary dependencies with npm:

```bash
npm install
```

Then, install Playwright:

```bash
npx playwright install
```

Additionally, install required packages for parsing sitemaps:
```
 npm install xml2js axios
```

## Usage

Run the crawler with:

```bash
node crawler.js
```

### Input Options

After running the script, you will be prompted to choose one of the following options:

1. **Crawl URLs from a `txt` file** â€“ Reads URLs from `URLs/urls.txt`.
2. **Crawl URLs from a sitemap** â€“ Extracts URLs from a provided XML sitemap.
3. **Crawl both sources** â€“ Combines both methods.

#### Using a `txt` File

To use URLs from a text file, place them in:

```
URLs/urls.txt
```

Each URL should be on a new line.

#### Using a Sitemap

If you choose to crawl from a sitemap, you need to provide a valid URL, such as:

```
https://example.com/sitemap.xml
```

The crawler will parse the sitemap and extract all listed URLs.

### Cookie Configuration

If you need to pass a specific cookie, modify the `cookie` object inside `crawler.js`:

```javascript
const cookie = {
    name: 'name',
    value: 'value',
    domain: '.' + host,
    path: '/',
    httpOnly: true,
    secure: false,
};
```

Make sure to add it to the browser context:

```javascript
await context.addCookies([cookie]);
```

### Timeout Settings

You can modify the default timeout settings inside `crawler.js`:

```javascript
page.setDefaultTimeout(60000);
page.setDefaultNavigationTimeout(60000);
```

### Output Files

Results are saved in the `URLs/` folder:

- `urls-crawled.txt` â€“ Successfully crawled URLs.
- `urls-failed.txt` â€“ URLs that failed to load.
- `urls-404.txt` â€“ URLs that returned a `404 Not Found` response.
- `urls-500.txt` â€“ URLs that returned a `500 Internal Server Error` response.
- `slowest-pages.txt` â€“ List of the slowest 10% of pages with their load times.

### Performance Analysis

The crawler now includes performance monitoring features:

- Tracks individual page load times.
- Calculates average load time across all successfully loaded pages.
- Identifies and reports the slowest 10% of pages.
- Format of slowest-pages.txt: "URL - time in seconds".

### Summary Report

At the end of each run, a summary is displayed, including:

- Total sites crawled
- Successfully loaded sites
- Sites with timeouts
- Sites with errors
- `404 Not Found` pages
- `500 Internal Server Error` pages
- Average page load time
- Total execution time

## Notes

- Ensure the `URLs/` folder exists before running the crawler.
- If a URL file or sitemap is missing, the program will prompt you accordingly.
- Load times are measured from the start of navigation until the DOM content is loaded.
- The slowest pages report helps identify potential performance bottlenecks.
---

Happy crawling! ðŸš€
